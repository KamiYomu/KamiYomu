services:
    kamiyomu.web:
      image: ${DOCKER_REGISTRY-}kamiyomuweb
      build:
        context: .
        dockerfile: KamiYomu.Web/Dockerfile
      environment:
        # List of Hangfire server identifiers available to process jobs.
        # Each name corresponds to a distinct background worker instance;
        # add more entries here if you want multiple servers to share or divide queues.
        Worker__ServerAvailableNames__0:   "KamiYomu-background-1" 
          
        # Queues dedicated to downloading individual chapters.
        Worker__DownloadChapterQueues__0:  "download-chapter-queue-1" 

        # Queues dedicated to scheduling manga downloads (manages chapter download jobs).
        Worker__MangaDownloadSchedulerQueues__0:  "manga-download-scheduler-queue-1" 

        # Queues dedicated to discovering new chapters (polling or scraping for updates).
        Worker__DiscoveryNewChapterQueues__0:  "discovery-new-chapter-queue-1" 

        # Specifies the number of background processing threads Hangfire will spawn.
        # Increasing this value allows more jobs to run concurrently, but also raises CPU load 
        # and memory usage.
        # Each worker consumes ~80 MB of memory on average while active 
        # (actual usage may vary depending on the crawler agent implementation and system configuration).
        Worker__WorkerCount: 4

        # Defines the maximum number of crawler instances allowed to run concurrently for the same source.
        # Typically set to 1 to ensure only a single crawler operates at a time, preventing duplicate work,
        # resource conflicts, and potential rate‑limiting or blocking by the target system.
        # This value can be increased to improve throughput if the source supports multiple concurrent requests.
        #
        # Note:
        # - Worker__WorkerCount controls the total number of threads available.
        # - Worker__MaxConcurrentCrawlerInstances limits how many threads can be used by the same crawler.
        #
        # Examples:
        # - If Worker__MaxConcurrentCrawlerInstances = 1 and Worker__WorkerCount = 4,
        #   then up to 4 different crawler agents can run independently.
        # - If Worker__MaxConcurrentCrawlerInstances = 2 and Worker__WorkerCount = 6,
        #   then each crawler agent can run up to 2 instances concurrently,
        #   while up to 3 different crawler agents may be active at the same time.
        Worker__MaxConcurrentCrawlerInstances: 2

        # Minimum delay (in milliseconds) between job executions.
        # Helps throttle requests to external services and avoid hitting rate limits (e.g., HTTP 423 "Too Many Requests").
        Worker__MinWaitPeriodInMilliseconds: 3000

        # Maximum delay (in milliseconds) between job executions.
        # Provides variability in scheduling to reduce the chance of IP blocking or service throttling.
        Worker__MaxWaitPeriodInMilliseconds: 9001

        # Maximum number of retry attempts for failed jobs before marking them as permanently failed.
        Worker__MaxRetryAttempts: 10

        # Default language for the web interface (e.g., "en", "pt-BR", "fr").
        UI__DefaultLanguage: "en" 
      volumes:
        - ./AppData/manga:/manga
        - kamiyomu_database:/db 
        - ./AppData/agents:/agents 
        - kamiyomu_logs:/logs 
      healthcheck:
        test: ["CMD", "curl", "-f", "https://localhost:5001/healthz"]
        interval: 30s
        timeout: 10s
        retries: 3

volumes:
  kamiyomu_agents:
  kamiyomu_database:
  kamiyomu_logs:


